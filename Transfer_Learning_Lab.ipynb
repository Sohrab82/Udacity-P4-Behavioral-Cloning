{
 "cells": [
  {
   "source": [
    "### Import libraries\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import numpy as np\n",
    "import cv2\n"
   ]
  },
  {
   "source": [
    "### Read the input data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_name):\n",
    "    image = cv2.imread(file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/driving_log.csv') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "samples = []\n",
    "for i, line in enumerate(lines[1:]):\n",
    "    line = line.split(',')\n",
    "    # file name\n",
    "    file_name = './data/' + line[0].strip()\n",
    "    stearing_angle = float(line[3])\n",
    "    samples.append([file_name, stearing_angle, 0])\n",
    "\n",
    "    # add flipped image\n",
    "    samples.append([file_name, -stearing_angle, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        sklearn.utils.shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                file_name = batch_sample[0]\n",
    "                image = cv2.imread(file_name)\n",
    "                stearing_angle = batch_sample[1]\n",
    "                flip_st = batch_sample[2]\n",
    "                if flip_st == 1:\n",
    "                    image = np.fliplr(image)\n",
    "                # print(image[60:-20,:].shape)\n",
    "                images.append(image[60:-20,:])\n",
    "                angles.append(stearing_angle)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "\n",
    "            yield (X_train, y_train)\n",
    "            \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "\n",
    "# Set our batch size\n",
    "batch_size = 32\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = generator(validation_samples, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_35 (Lambda)           (None, 80, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 76801     \n",
      "=================================================================\n",
      "Total params: 76,801\n",
      "Trainable params: 76,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 57.3400 - val_loss: 0.0461\n",
      "Epoch 2/5\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.0411 - val_loss: 0.0263\n",
      "Epoch 3/5\n",
      "402/402 [==============================] - 8s 20ms/step - loss: 0.0444 - val_loss: 0.1124\n",
      "Epoch 4/5\n",
      "402/402 [==============================] - 8s 20ms/step - loss: 0.0503 - val_loss: 0.0718\n",
      "Epoch 5/5\n",
      "402/402 [==============================] - 8s 21ms/step - loss: 0.1927 - val_loss: 0.7995\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f543c07aa30>"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Lambda, Input, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "\n",
    "ch, row, col = 3, 80, 320  # Trimmed image format\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Input(shape=(row, col, ch)))\n",
    "model.add(Lambda(lambda x: x/255.,\n",
    "        input_shape=(row, col, ch),\n",
    "        output_shape=(row, col, ch)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()\n",
    "model.fit(train_generator,\n",
    "            steps_per_epoch=np.ceil(len(train_samples)/batch_size),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=np.ceil(len(validation_samples)/batch_size),\n",
    "            epochs=5, verbose=1)\n",
    "            "
   ]
  },
  {
   "source": [
    "### Define some variables to be used later"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freeze_flag = True  # `True` to freeze layers, `False` for full training\n",
    "weights_flag = 'imagenet' # 'imagenet' or None\n",
    "preprocess_flag = True # Should be true for ImageNet pre-trained typically\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Load the pretrained model: \n",
    "    VGG16 (keras.applications.vgg16), \n",
    "    ResNet50 (keras.applications.resnet50), \n",
    "    Inception_V3 (keras.applications.inception_v3), ...\n",
    "full list of models: https://keras.io/api/applications/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# We can use smaller than the default 299x299x3 input for InceptionV3\n",
    "# which will speed up training. Keras v2.0.9 supports down to 139x139x3\n",
    "input_size = 299\n",
    "model = InceptionV3(weights=weights_flag, include_top=True,\n",
    "                        input_shape=(input_size, input_size, 3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Predict with the pretrained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted: [('n01871265', 'tusker', 0.64368755), ('n02504458', 'African_elephant', 0.17799999), ('n02504013', 'Indian_elephant', 0.082684584)]\n"
     ]
    }
   ],
   "source": [
    "# Predict with loaded model\n",
    "img_path = './images/elephant.jpg'\n",
    "img = image.load_img(img_path, target_size=(input_size, input_size))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# Perform inference on our pre-processed image\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Check the top 3 predictions of the model\n",
    "print('Predicted:', decode_predictions(predictions, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained with dropped top layers\n",
    "\n",
    "In order to change the input_size to 139x139x3, `include_top` must be set to `False`, which means the final fully-connected layer with 1,000 nodes for each ImageNet class is dropped, as well as a Global Average Pooling layer.\n",
    "\n",
    "You can freeze layers by setting `layer.trainable` to False for a given `layer`. Within a `model`, you can get the list of layers with `model.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"inception_v3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 139, 139, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_94 (Conv2D)              (None, 69, 69, 32)   864         input_2[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_94 (BatchNo (None, 69, 69, 32)   96          conv2d_94[0][0]                  \n__________________________________________________________________________________________________\nactivation_94 (Activation)      (None, 69, 69, 32)   0           batch_normalization_94[0][0]     \n__________________________________________________________________________________________________\nconv2d_95 (Conv2D)              (None, 67, 67, 32)   9216        activation_94[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_95 (BatchNo (None, 67, 67, 32)   96          conv2d_95[0][0]                  \n__________________________________________________________________________________________________\nactivation_95 (Activation)      (None, 67, 67, 32)   0           batch_normalization_95[0][0]     \n__________________________________________________________________________________________________\nconv2d_96 (Conv2D)              (None, 67, 67, 64)   18432       activation_95[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_96 (BatchNo (None, 67, 67, 64)   192         conv2d_96[0][0]                  \n__________________________________________________________________________________________________\nactivation_96 (Activation)      (None, 67, 67, 64)   0           batch_normalization_96[0][0]     \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 33, 33, 64)   0           activation_96[0][0]              \n__________________________________________________________________________________________________\nconv2d_97 (Conv2D)              (None, 33, 33, 80)   5120        max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_97 (BatchNo (None, 33, 33, 80)   240         conv2d_97[0][0]                  \n__________________________________________________________________________________________________\nactivation_97 (Activation)      (None, 33, 33, 80)   0           batch_normalization_97[0][0]     \n__________________________________________________________________________________________________\nconv2d_98 (Conv2D)              (None, 31, 31, 192)  138240      activation_97[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_98 (BatchNo (None, 31, 31, 192)  576         conv2d_98[0][0]                  \n__________________________________________________________________________________________________\nactivation_98 (Activation)      (None, 31, 31, 192)  0           batch_normalization_98[0][0]     \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 15, 15, 192)  0           activation_98[0][0]              \n__________________________________________________________________________________________________\nconv2d_102 (Conv2D)             (None, 15, 15, 64)   12288       max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_102 (BatchN (None, 15, 15, 64)   192         conv2d_102[0][0]                 \n__________________________________________________________________________________________________\nactivation_102 (Activation)     (None, 15, 15, 64)   0           batch_normalization_102[0][0]    \n__________________________________________________________________________________________________\nconv2d_100 (Conv2D)             (None, 15, 15, 48)   9216        max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nconv2d_103 (Conv2D)             (None, 15, 15, 96)   55296       activation_102[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_100 (BatchN (None, 15, 15, 48)   144         conv2d_100[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_103 (BatchN (None, 15, 15, 96)   288         conv2d_103[0][0]                 \n__________________________________________________________________________________________________\nactivation_100 (Activation)     (None, 15, 15, 48)   0           batch_normalization_100[0][0]    \n__________________________________________________________________________________________________\nactivation_103 (Activation)     (None, 15, 15, 96)   0           batch_normalization_103[0][0]    \n__________________________________________________________________________________________________\naverage_pooling2d_9 (AveragePoo (None, 15, 15, 192)  0           max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nconv2d_99 (Conv2D)              (None, 15, 15, 64)   12288       max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nconv2d_101 (Conv2D)             (None, 15, 15, 64)   76800       activation_100[0][0]             \n__________________________________________________________________________________________________\nconv2d_104 (Conv2D)             (None, 15, 15, 96)   82944       activation_103[0][0]             \n__________________________________________________________________________________________________\nconv2d_105 (Conv2D)             (None, 15, 15, 32)   6144        average_pooling2d_9[0][0]        \n__________________________________________________________________________________________________\nbatch_normalization_99 (BatchNo (None, 15, 15, 64)   192         conv2d_99[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_101 (BatchN (None, 15, 15, 64)   192         conv2d_101[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_104 (BatchN (None, 15, 15, 96)   288         conv2d_104[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_105 (BatchN (None, 15, 15, 32)   96          conv2d_105[0][0]                 \n__________________________________________________________________________________________________\nactivation_99 (Activation)      (None, 15, 15, 64)   0           batch_normalization_99[0][0]     \n__________________________________________________________________________________________________\nactivation_101 (Activation)     (None, 15, 15, 64)   0           batch_normalization_101[0][0]    \n__________________________________________________________________________________________________\nactivation_104 (Activation)     (None, 15, 15, 96)   0           batch_normalization_104[0][0]    \n__________________________________________________________________________________________________\nactivation_105 (Activation)     (None, 15, 15, 32)   0           batch_normalization_105[0][0]    \n__________________________________________________________________________________________________\nmixed0 (Concatenate)            (None, 15, 15, 256)  0           activation_99[0][0]              \n                                                                 activation_101[0][0]             \n                                                                 activation_104[0][0]             \n                                                                 activation_105[0][0]             \n__________________________________________________________________________________________________\nconv2d_109 (Conv2D)             (None, 15, 15, 64)   16384       mixed0[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_109 (BatchN (None, 15, 15, 64)   192         conv2d_109[0][0]                 \n__________________________________________________________________________________________________\nactivation_109 (Activation)     (None, 15, 15, 64)   0           batch_normalization_109[0][0]    \n__________________________________________________________________________________________________\nconv2d_107 (Conv2D)             (None, 15, 15, 48)   12288       mixed0[0][0]                     \n__________________________________________________________________________________________________\nconv2d_110 (Conv2D)             (None, 15, 15, 96)   55296       activation_109[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_107 (BatchN (None, 15, 15, 48)   144         conv2d_107[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_110 (BatchN (None, 15, 15, 96)   288         conv2d_110[0][0]                 \n__________________________________________________________________________________________________\nactivation_107 (Activation)     (None, 15, 15, 48)   0           batch_normalization_107[0][0]    \n__________________________________________________________________________________________________\nactivation_110 (Activation)     (None, 15, 15, 96)   0           batch_normalization_110[0][0]    \n__________________________________________________________________________________________________\naverage_pooling2d_10 (AveragePo (None, 15, 15, 256)  0           mixed0[0][0]                     \n__________________________________________________________________________________________________\nconv2d_106 (Conv2D)             (None, 15, 15, 64)   16384       mixed0[0][0]                     \n__________________________________________________________________________________________________\nconv2d_108 (Conv2D)             (None, 15, 15, 64)   76800       activation_107[0][0]             \n__________________________________________________________________________________________________\nconv2d_111 (Conv2D)             (None, 15, 15, 96)   82944       activation_110[0][0]             \n__________________________________________________________________________________________________\nconv2d_112 (Conv2D)             (None, 15, 15, 64)   16384       average_pooling2d_10[0][0]       \n__________________________________________________________________________________________________\nbatch_normalization_106 (BatchN (None, 15, 15, 64)   192         conv2d_106[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_108 (BatchN (None, 15, 15, 64)   192         conv2d_108[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_111 (BatchN (None, 15, 15, 96)   288         conv2d_111[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_112 (BatchN (None, 15, 15, 64)   192         conv2d_112[0][0]                 \n__________________________________________________________________________________________________\nactivation_106 (Activation)     (None, 15, 15, 64)   0           batch_normalization_106[0][0]    \n__________________________________________________________________________________________________\nactivation_108 (Activation)     (None, 15, 15, 64)   0           batch_normalization_108[0][0]    \n__________________________________________________________________________________________________\nactivation_111 (Activation)     (None, 15, 15, 96)   0           batch_normalization_111[0][0]    \n__________________________________________________________________________________________________\nactivation_112 (Activation)     (None, 15, 15, 64)   0           batch_normalization_112[0][0]    \n__________________________________________________________________________________________________\nmixed1 (Concatenate)            (None, 15, 15, 288)  0           activation_106[0][0]             \n                                                                 activation_108[0][0]             \n                                                                 activation_111[0][0]             \n                                                                 activation_112[0][0]             \n__________________________________________________________________________________________________\nconv2d_116 (Conv2D)             (None, 15, 15, 64)   18432       mixed1[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_116 (BatchN (None, 15, 15, 64)   192         conv2d_116[0][0]                 \n__________________________________________________________________________________________________\nactivation_116 (Activation)     (None, 15, 15, 64)   0           batch_normalization_116[0][0]    \n__________________________________________________________________________________________________\nconv2d_114 (Conv2D)             (None, 15, 15, 48)   13824       mixed1[0][0]                     \n__________________________________________________________________________________________________\nconv2d_117 (Conv2D)             (None, 15, 15, 96)   55296       activation_116[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_114 (BatchN (None, 15, 15, 48)   144         conv2d_114[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_117 (BatchN (None, 15, 15, 96)   288         conv2d_117[0][0]                 \n__________________________________________________________________________________________________\nactivation_114 (Activation)     (None, 15, 15, 48)   0           batch_normalization_114[0][0]    \n__________________________________________________________________________________________________\nactivation_117 (Activation)     (None, 15, 15, 96)   0           batch_normalization_117[0][0]    \n__________________________________________________________________________________________________\naverage_pooling2d_11 (AveragePo (None, 15, 15, 288)  0           mixed1[0][0]                     \n__________________________________________________________________________________________________\nconv2d_113 (Conv2D)             (None, 15, 15, 64)   18432       mixed1[0][0]                     \n__________________________________________________________________________________________________\nconv2d_115 (Conv2D)             (None, 15, 15, 64)   76800       activation_114[0][0]             \n__________________________________________________________________________________________________\nconv2d_118 (Conv2D)             (None, 15, 15, 96)   82944       activation_117[0][0]             \n__________________________________________________________________________________________________\nconv2d_119 (Conv2D)             (None, 15, 15, 64)   18432       average_pooling2d_11[0][0]       \n__________________________________________________________________________________________________\nbatch_normalization_113 (BatchN (None, 15, 15, 64)   192         conv2d_113[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_115 (BatchN (None, 15, 15, 64)   192         conv2d_115[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_118 (BatchN (None, 15, 15, 96)   288         conv2d_118[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_119 (BatchN (None, 15, 15, 64)   192         conv2d_119[0][0]                 \n__________________________________________________________________________________________________\nactivation_113 (Activation)     (None, 15, 15, 64)   0           batch_normalization_113[0][0]    \n__________________________________________________________________________________________________\nactivation_115 (Activation)     (None, 15, 15, 64)   0           batch_normalization_115[0][0]    \n__________________________________________________________________________________________________\nactivation_118 (Activation)     (None, 15, 15, 96)   0           batch_normalization_118[0][0]    \n__________________________________________________________________________________________________\nactivation_119 (Activation)     (None, 15, 15, 64)   0           batch_normalization_119[0][0]    \n__________________________________________________________________________________________________\nmixed2 (Concatenate)            (None, 15, 15, 288)  0           activation_113[0][0]             \n                                                                 activation_115[0][0]             \n                                                                 activation_118[0][0]             \n                                                                 activation_119[0][0]             \n__________________________________________________________________________________________________\nconv2d_121 (Conv2D)             (None, 15, 15, 64)   18432       mixed2[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_121 (BatchN (None, 15, 15, 64)   192         conv2d_121[0][0]                 \n__________________________________________________________________________________________________\nactivation_121 (Activation)     (None, 15, 15, 64)   0           batch_normalization_121[0][0]    \n__________________________________________________________________________________________________\nconv2d_122 (Conv2D)             (None, 15, 15, 96)   55296       activation_121[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_122 (BatchN (None, 15, 15, 96)   288         conv2d_122[0][0]                 \n__________________________________________________________________________________________________\nactivation_122 (Activation)     (None, 15, 15, 96)   0           batch_normalization_122[0][0]    \n__________________________________________________________________________________________________\nconv2d_120 (Conv2D)             (None, 7, 7, 384)    995328      mixed2[0][0]                     \n__________________________________________________________________________________________________\nconv2d_123 (Conv2D)             (None, 7, 7, 96)     82944       activation_122[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_120 (BatchN (None, 7, 7, 384)    1152        conv2d_120[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_123 (BatchN (None, 7, 7, 96)     288         conv2d_123[0][0]                 \n__________________________________________________________________________________________________\nactivation_120 (Activation)     (None, 7, 7, 384)    0           batch_normalization_120[0][0]    \n__________________________________________________________________________________________________\nactivation_123 (Activation)     (None, 7, 7, 96)     0           batch_normalization_123[0][0]    \n__________________________________________________________________________________________________\nmax_pooling2d_6 (MaxPooling2D)  (None, 7, 7, 288)    0           mixed2[0][0]                     \n__________________________________________________________________________________________________\nmixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_120[0][0]             \n                                                                 activation_123[0][0]             \n                                                                 max_pooling2d_6[0][0]            \n__________________________________________________________________________________________________\nconv2d_128 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_128 (BatchN (None, 7, 7, 128)    384         conv2d_128[0][0]                 \n__________________________________________________________________________________________________\nactivation_128 (Activation)     (None, 7, 7, 128)    0           batch_normalization_128[0][0]    \n__________________________________________________________________________________________________\nconv2d_129 (Conv2D)             (None, 7, 7, 128)    114688      activation_128[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_129 (BatchN (None, 7, 7, 128)    384         conv2d_129[0][0]                 \n__________________________________________________________________________________________________\nactivation_129 (Activation)     (None, 7, 7, 128)    0           batch_normalization_129[0][0]    \n__________________________________________________________________________________________________\nconv2d_125 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n__________________________________________________________________________________________________\nconv2d_130 (Conv2D)             (None, 7, 7, 128)    114688      activation_129[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_125 (BatchN (None, 7, 7, 128)    384         conv2d_125[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_130 (BatchN (None, 7, 7, 128)    384         conv2d_130[0][0]                 \n__________________________________________________________________________________________________\nactivation_125 (Activation)     (None, 7, 7, 128)    0           batch_normalization_125[0][0]    \n__________________________________________________________________________________________________\nactivation_130 (Activation)     (None, 7, 7, 128)    0           batch_normalization_130[0][0]    \n__________________________________________________________________________________________________\nconv2d_126 (Conv2D)             (None, 7, 7, 128)    114688      activation_125[0][0]             \n__________________________________________________________________________________________________\nconv2d_131 (Conv2D)             (None, 7, 7, 128)    114688      activation_130[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_126 (BatchN (None, 7, 7, 128)    384         conv2d_126[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_131 (BatchN (None, 7, 7, 128)    384         conv2d_131[0][0]                 \n__________________________________________________________________________________________________\nactivation_126 (Activation)     (None, 7, 7, 128)    0           batch_normalization_126[0][0]    \n__________________________________________________________________________________________________\nactivation_131 (Activation)     (None, 7, 7, 128)    0           batch_normalization_131[0][0]    \n__________________________________________________________________________________________________\naverage_pooling2d_12 (AveragePo (None, 7, 7, 768)    0           mixed3[0][0]                     \n__________________________________________________________________________________________________\nconv2d_124 (Conv2D)             (None, 7, 7, 192)    147456      mixed3[0][0]                     \n__________________________________________________________________________________________________\nconv2d_127 (Conv2D)             (None, 7, 7, 192)    172032      activation_126[0][0]             \n__________________________________________________________________________________________________\nconv2d_132 (Conv2D)             (None, 7, 7, 192)    172032      activation_131[0][0]             \n__________________________________________________________________________________________________\nconv2d_133 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_12[0][0]       \n__________________________________________________________________________________________________\nbatch_normalization_124 (BatchN (None, 7, 7, 192)    576         conv2d_124[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_127 (BatchN (None, 7, 7, 192)    576         conv2d_127[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_132 (BatchN (None, 7, 7, 192)    576         conv2d_132[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_133 (BatchN (None, 7, 7, 192)    576         conv2d_133[0][0]                 \n__________________________________________________________________________________________________\nactivation_124 (Activation)     (None, 7, 7, 192)    0           batch_normalization_124[0][0]    \n__________________________________________________________________________________________________\nactivation_127 (Activation)     (None, 7, 7, 192)    0           batch_normalization_127[0][0]    \n__________________________________________________________________________________________________\nactivation_132 (Activation)     (None, 7, 7, 192)    0           batch_normalization_132[0][0]    \n__________________________________________________________________________________________________\nactivation_133 (Activation)     (None, 7, 7, 192)    0           batch_normalization_133[0][0]    \n__________________________________________________________________________________________________\nmixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_124[0][0]             \n                                                                 activation_127[0][0]             \n                                                                 activation_132[0][0]             \n                                                                 activation_133[0][0]             \n__________________________________________________________________________________________________\nconv2d_138 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_138 (BatchN (None, 7, 7, 160)    480         conv2d_138[0][0]                 \n__________________________________________________________________________________________________\nactivation_138 (Activation)     (None, 7, 7, 160)    0           batch_normalization_138[0][0]    \n__________________________________________________________________________________________________\nconv2d_139 (Conv2D)             (None, 7, 7, 160)    179200      activation_138[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_139 (BatchN (None, 7, 7, 160)    480         conv2d_139[0][0]                 \n__________________________________________________________________________________________________\nactivation_139 (Activation)     (None, 7, 7, 160)    0           batch_normalization_139[0][0]    \n__________________________________________________________________________________________________\nconv2d_135 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n__________________________________________________________________________________________________\nconv2d_140 (Conv2D)             (None, 7, 7, 160)    179200      activation_139[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_135 (BatchN (None, 7, 7, 160)    480         conv2d_135[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_140 (BatchN (None, 7, 7, 160)    480         conv2d_140[0][0]                 \n__________________________________________________________________________________________________\nactivation_135 (Activation)     (None, 7, 7, 160)    0           batch_normalization_135[0][0]    \n__________________________________________________________________________________________________\nactivation_140 (Activation)     (None, 7, 7, 160)    0           batch_normalization_140[0][0]    \n__________________________________________________________________________________________________\nconv2d_136 (Conv2D)             (None, 7, 7, 160)    179200      activation_135[0][0]             \n__________________________________________________________________________________________________\nconv2d_141 (Conv2D)             (None, 7, 7, 160)    179200      activation_140[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_136 (BatchN (None, 7, 7, 160)    480         conv2d_136[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_141 (BatchN (None, 7, 7, 160)    480         conv2d_141[0][0]                 \n__________________________________________________________________________________________________\nactivation_136 (Activation)     (None, 7, 7, 160)    0           batch_normalization_136[0][0]    \n__________________________________________________________________________________________________\nactivation_141 (Activation)     (None, 7, 7, 160)    0           batch_normalization_141[0][0]    \n__________________________________________________________________________________________________\naverage_pooling2d_13 (AveragePo (None, 7, 7, 768)    0           mixed4[0][0]                     \n__________________________________________________________________________________________________\nconv2d_134 (Conv2D)             (None, 7, 7, 192)    147456      mixed4[0][0]                     \n__________________________________________________________________________________________________\nconv2d_137 (Conv2D)             (None, 7, 7, 192)    215040      activation_136[0][0]             \n__________________________________________________________________________________________________\nconv2d_142 (Conv2D)             (None, 7, 7, 192)    215040      activation_141[0][0]             \n__________________________________________________________________________________________________\nconv2d_143 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_13[0][0]       \n__________________________________________________________________________________________________\nbatch_normalization_134 (BatchN (None, 7, 7, 192)    576         conv2d_134[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_137 (BatchN (None, 7, 7, 192)    576         conv2d_137[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_142 (BatchN (None, 7, 7, 192)    576         conv2d_142[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_143 (BatchN (None, 7, 7, 192)    576         conv2d_143[0][0]                 \n__________________________________________________________________________________________________\nactivation_134 (Activation)     (None, 7, 7, 192)    0           batch_normalization_134[0][0]    \n__________________________________________________________________________________________________\nactivation_137 (Activation)     (None, 7, 7, 192)    0           batch_normalization_137[0][0]    \n__________________________________________________________________________________________________\nactivation_142 (Activation)     (None, 7, 7, 192)    0           batch_normalization_142[0][0]    \n__________________________________________________________________________________________________\nactivation_143 (Activation)     (None, 7, 7, 192)    0           batch_normalization_143[0][0]    \n__________________________________________________________________________________________________\nmixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_134[0][0]             \n                                                                 activation_137[0][0]             \n                                                                 activation_142[0][0]             \n                                                                 activation_143[0][0]             \n__________________________________________________________________________________________________\nconv2d_148 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_148 (BatchN (None, 7, 7, 160)    480         conv2d_148[0][0]                 \n__________________________________________________________________________________________________\nactivation_148 (Activation)     (None, 7, 7, 160)    0           batch_normalization_148[0][0]    \n__________________________________________________________________________________________________\nconv2d_149 (Conv2D)             (None, 7, 7, 160)    179200      activation_148[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_149 (BatchN (None, 7, 7, 160)    480         conv2d_149[0][0]                 \n__________________________________________________________________________________________________\nactivation_149 (Activation)     (None, 7, 7, 160)    0           batch_normalization_149[0][0]    \n__________________________________________________________________________________________________\nconv2d_145 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n__________________________________________________________________________________________________\nconv2d_150 (Conv2D)             (None, 7, 7, 160)    179200      activation_149[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_145 (BatchN (None, 7, 7, 160)    480         conv2d_145[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_150 (BatchN (None, 7, 7, 160)    480         conv2d_150[0][0]                 \n__________________________________________________________________________________________________\nactivation_145 (Activation)     (None, 7, 7, 160)    0           batch_normalization_145[0][0]    \n__________________________________________________________________________________________________\nactivation_150 (Activation)     (None, 7, 7, 160)    0           batch_normalization_150[0][0]    \n__________________________________________________________________________________________________\nconv2d_146 (Conv2D)             (None, 7, 7, 160)    179200      activation_145[0][0]             \n__________________________________________________________________________________________________\nconv2d_151 (Conv2D)             (None, 7, 7, 160)    179200      activation_150[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_146 (BatchN (None, 7, 7, 160)    480         conv2d_146[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_151 (BatchN (None, 7, 7, 160)    480         conv2d_151[0][0]                 \n__________________________________________________________________________________________________\nactivation_146 (Activation)     (None, 7, 7, 160)    0           batch_normalization_146[0][0]    \n__________________________________________________________________________________________________\nactivation_151 (Activation)     (None, 7, 7, 160)    0           batch_normalization_151[0][0]    \n__________________________________________________________________________________________________\naverage_pooling2d_14 (AveragePo (None, 7, 7, 768)    0           mixed5[0][0]                     \n__________________________________________________________________________________________________\nconv2d_144 (Conv2D)             (None, 7, 7, 192)    147456      mixed5[0][0]                     \n__________________________________________________________________________________________________\nconv2d_147 (Conv2D)             (None, 7, 7, 192)    215040      activation_146[0][0]             \n__________________________________________________________________________________________________\nconv2d_152 (Conv2D)             (None, 7, 7, 192)    215040      activation_151[0][0]             \n__________________________________________________________________________________________________\nconv2d_153 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_14[0][0]       \n__________________________________________________________________________________________________\nbatch_normalization_144 (BatchN (None, 7, 7, 192)    576         conv2d_144[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_147 (BatchN (None, 7, 7, 192)    576         conv2d_147[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_152 (BatchN (None, 7, 7, 192)    576         conv2d_152[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_153 (BatchN (None, 7, 7, 192)    576         conv2d_153[0][0]                 \n__________________________________________________________________________________________________\nactivation_144 (Activation)     (None, 7, 7, 192)    0           batch_normalization_144[0][0]    \n__________________________________________________________________________________________________\nactivation_147 (Activation)     (None, 7, 7, 192)    0           batch_normalization_147[0][0]    \n__________________________________________________________________________________________________\nactivation_152 (Activation)     (None, 7, 7, 192)    0           batch_normalization_152[0][0]    \n__________________________________________________________________________________________________\nactivation_153 (Activation)     (None, 7, 7, 192)    0           batch_normalization_153[0][0]    \n__________________________________________________________________________________________________\nmixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_144[0][0]             \n                                                                 activation_147[0][0]             \n                                                                 activation_152[0][0]             \n                                                                 activation_153[0][0]             \n__________________________________________________________________________________________________\nconv2d_158 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_158 (BatchN (None, 7, 7, 192)    576         conv2d_158[0][0]                 \n__________________________________________________________________________________________________\nactivation_158 (Activation)     (None, 7, 7, 192)    0           batch_normalization_158[0][0]    \n__________________________________________________________________________________________________\nconv2d_159 (Conv2D)             (None, 7, 7, 192)    258048      activation_158[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_159 (BatchN (None, 7, 7, 192)    576         conv2d_159[0][0]                 \n__________________________________________________________________________________________________\nactivation_159 (Activation)     (None, 7, 7, 192)    0           batch_normalization_159[0][0]    \n__________________________________________________________________________________________________\nconv2d_155 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n__________________________________________________________________________________________________\nconv2d_160 (Conv2D)             (None, 7, 7, 192)    258048      activation_159[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_155 (BatchN (None, 7, 7, 192)    576         conv2d_155[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_160 (BatchN (None, 7, 7, 192)    576         conv2d_160[0][0]                 \n__________________________________________________________________________________________________\nactivation_155 (Activation)     (None, 7, 7, 192)    0           batch_normalization_155[0][0]    \n__________________________________________________________________________________________________\nactivation_160 (Activation)     (None, 7, 7, 192)    0           batch_normalization_160[0][0]    \n__________________________________________________________________________________________________\nconv2d_156 (Conv2D)             (None, 7, 7, 192)    258048      activation_155[0][0]             \n__________________________________________________________________________________________________\nconv2d_161 (Conv2D)             (None, 7, 7, 192)    258048      activation_160[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_156 (BatchN (None, 7, 7, 192)    576         conv2d_156[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_161 (BatchN (None, 7, 7, 192)    576         conv2d_161[0][0]                 \n__________________________________________________________________________________________________\nactivation_156 (Activation)     (None, 7, 7, 192)    0           batch_normalization_156[0][0]    \n__________________________________________________________________________________________________\nactivation_161 (Activation)     (None, 7, 7, 192)    0           batch_normalization_161[0][0]    \n__________________________________________________________________________________________________\naverage_pooling2d_15 (AveragePo (None, 7, 7, 768)    0           mixed6[0][0]                     \n__________________________________________________________________________________________________\nconv2d_154 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n__________________________________________________________________________________________________\nconv2d_157 (Conv2D)             (None, 7, 7, 192)    258048      activation_156[0][0]             \n__________________________________________________________________________________________________\nconv2d_162 (Conv2D)             (None, 7, 7, 192)    258048      activation_161[0][0]             \n__________________________________________________________________________________________________\nconv2d_163 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_15[0][0]       \n__________________________________________________________________________________________________\nbatch_normalization_154 (BatchN (None, 7, 7, 192)    576         conv2d_154[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_157 (BatchN (None, 7, 7, 192)    576         conv2d_157[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_162 (BatchN (None, 7, 7, 192)    576         conv2d_162[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_163 (BatchN (None, 7, 7, 192)    576         conv2d_163[0][0]                 \n__________________________________________________________________________________________________\nactivation_154 (Activation)     (None, 7, 7, 192)    0           batch_normalization_154[0][0]    \n__________________________________________________________________________________________________\nactivation_157 (Activation)     (None, 7, 7, 192)    0           batch_normalization_157[0][0]    \n__________________________________________________________________________________________________\nactivation_162 (Activation)     (None, 7, 7, 192)    0           batch_normalization_162[0][0]    \n__________________________________________________________________________________________________\nactivation_163 (Activation)     (None, 7, 7, 192)    0           batch_normalization_163[0][0]    \n__________________________________________________________________________________________________\nmixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_154[0][0]             \n                                                                 activation_157[0][0]             \n                                                                 activation_162[0][0]             \n                                                                 activation_163[0][0]             \n__________________________________________________________________________________________________\nconv2d_166 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_166 (BatchN (None, 7, 7, 192)    576         conv2d_166[0][0]                 \n__________________________________________________________________________________________________\nactivation_166 (Activation)     (None, 7, 7, 192)    0           batch_normalization_166[0][0]    \n__________________________________________________________________________________________________\nconv2d_167 (Conv2D)             (None, 7, 7, 192)    258048      activation_166[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_167 (BatchN (None, 7, 7, 192)    576         conv2d_167[0][0]                 \n__________________________________________________________________________________________________\nactivation_167 (Activation)     (None, 7, 7, 192)    0           batch_normalization_167[0][0]    \n__________________________________________________________________________________________________\nconv2d_164 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n__________________________________________________________________________________________________\nconv2d_168 (Conv2D)             (None, 7, 7, 192)    258048      activation_167[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_164 (BatchN (None, 7, 7, 192)    576         conv2d_164[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_168 (BatchN (None, 7, 7, 192)    576         conv2d_168[0][0]                 \n__________________________________________________________________________________________________\nactivation_164 (Activation)     (None, 7, 7, 192)    0           batch_normalization_164[0][0]    \n__________________________________________________________________________________________________\nactivation_168 (Activation)     (None, 7, 7, 192)    0           batch_normalization_168[0][0]    \n__________________________________________________________________________________________________\nconv2d_165 (Conv2D)             (None, 3, 3, 320)    552960      activation_164[0][0]             \n__________________________________________________________________________________________________\nconv2d_169 (Conv2D)             (None, 3, 3, 192)    331776      activation_168[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_165 (BatchN (None, 3, 3, 320)    960         conv2d_165[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_169 (BatchN (None, 3, 3, 192)    576         conv2d_169[0][0]                 \n__________________________________________________________________________________________________\nactivation_165 (Activation)     (None, 3, 3, 320)    0           batch_normalization_165[0][0]    \n__________________________________________________________________________________________________\nactivation_169 (Activation)     (None, 3, 3, 192)    0           batch_normalization_169[0][0]    \n__________________________________________________________________________________________________\nmax_pooling2d_7 (MaxPooling2D)  (None, 3, 3, 768)    0           mixed7[0][0]                     \n__________________________________________________________________________________________________\nmixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_165[0][0]             \n                                                                 activation_169[0][0]             \n                                                                 max_pooling2d_7[0][0]            \n__________________________________________________________________________________________________\nconv2d_174 (Conv2D)             (None, 3, 3, 448)    573440      mixed8[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_174 (BatchN (None, 3, 3, 448)    1344        conv2d_174[0][0]                 \n__________________________________________________________________________________________________\nactivation_174 (Activation)     (None, 3, 3, 448)    0           batch_normalization_174[0][0]    \n__________________________________________________________________________________________________\nconv2d_171 (Conv2D)             (None, 3, 3, 384)    491520      mixed8[0][0]                     \n__________________________________________________________________________________________________\nconv2d_175 (Conv2D)             (None, 3, 3, 384)    1548288     activation_174[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_171 (BatchN (None, 3, 3, 384)    1152        conv2d_171[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_175 (BatchN (None, 3, 3, 384)    1152        conv2d_175[0][0]                 \n__________________________________________________________________________________________________\nactivation_171 (Activation)     (None, 3, 3, 384)    0           batch_normalization_171[0][0]    \n__________________________________________________________________________________________________\nactivation_175 (Activation)     (None, 3, 3, 384)    0           batch_normalization_175[0][0]    \n__________________________________________________________________________________________________\nconv2d_172 (Conv2D)             (None, 3, 3, 384)    442368      activation_171[0][0]             \n__________________________________________________________________________________________________\nconv2d_173 (Conv2D)             (None, 3, 3, 384)    442368      activation_171[0][0]             \n__________________________________________________________________________________________________\nconv2d_176 (Conv2D)             (None, 3, 3, 384)    442368      activation_175[0][0]             \n__________________________________________________________________________________________________\nconv2d_177 (Conv2D)             (None, 3, 3, 384)    442368      activation_175[0][0]             \n__________________________________________________________________________________________________\naverage_pooling2d_16 (AveragePo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n__________________________________________________________________________________________________\nconv2d_170 (Conv2D)             (None, 3, 3, 320)    409600      mixed8[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_172 (BatchN (None, 3, 3, 384)    1152        conv2d_172[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_173 (BatchN (None, 3, 3, 384)    1152        conv2d_173[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_176 (BatchN (None, 3, 3, 384)    1152        conv2d_176[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_177 (BatchN (None, 3, 3, 384)    1152        conv2d_177[0][0]                 \n__________________________________________________________________________________________________\nconv2d_178 (Conv2D)             (None, 3, 3, 192)    245760      average_pooling2d_16[0][0]       \n__________________________________________________________________________________________________\nbatch_normalization_170 (BatchN (None, 3, 3, 320)    960         conv2d_170[0][0]                 \n__________________________________________________________________________________________________\nactivation_172 (Activation)     (None, 3, 3, 384)    0           batch_normalization_172[0][0]    \n__________________________________________________________________________________________________\nactivation_173 (Activation)     (None, 3, 3, 384)    0           batch_normalization_173[0][0]    \n__________________________________________________________________________________________________\nactivation_176 (Activation)     (None, 3, 3, 384)    0           batch_normalization_176[0][0]    \n__________________________________________________________________________________________________\nactivation_177 (Activation)     (None, 3, 3, 384)    0           batch_normalization_177[0][0]    \n__________________________________________________________________________________________________\nbatch_normalization_178 (BatchN (None, 3, 3, 192)    576         conv2d_178[0][0]                 \n__________________________________________________________________________________________________\nactivation_170 (Activation)     (None, 3, 3, 320)    0           batch_normalization_170[0][0]    \n__________________________________________________________________________________________________\nmixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_172[0][0]             \n                                                                 activation_173[0][0]             \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 3, 3, 768)    0           activation_176[0][0]             \n                                                                 activation_177[0][0]             \n__________________________________________________________________________________________________\nactivation_178 (Activation)     (None, 3, 3, 192)    0           batch_normalization_178[0][0]    \n__________________________________________________________________________________________________\nmixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_170[0][0]             \n                                                                 mixed9_0[0][0]                   \n                                                                 concatenate_2[0][0]              \n                                                                 activation_178[0][0]             \n__________________________________________________________________________________________________\nconv2d_183 (Conv2D)             (None, 3, 3, 448)    917504      mixed9[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_183 (BatchN (None, 3, 3, 448)    1344        conv2d_183[0][0]                 \n__________________________________________________________________________________________________\nactivation_183 (Activation)     (None, 3, 3, 448)    0           batch_normalization_183[0][0]    \n__________________________________________________________________________________________________\nconv2d_180 (Conv2D)             (None, 3, 3, 384)    786432      mixed9[0][0]                     \n__________________________________________________________________________________________________\nconv2d_184 (Conv2D)             (None, 3, 3, 384)    1548288     activation_183[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_180 (BatchN (None, 3, 3, 384)    1152        conv2d_180[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_184 (BatchN (None, 3, 3, 384)    1152        conv2d_184[0][0]                 \n__________________________________________________________________________________________________\nactivation_180 (Activation)     (None, 3, 3, 384)    0           batch_normalization_180[0][0]    \n__________________________________________________________________________________________________\nactivation_184 (Activation)     (None, 3, 3, 384)    0           batch_normalization_184[0][0]    \n__________________________________________________________________________________________________\nconv2d_181 (Conv2D)             (None, 3, 3, 384)    442368      activation_180[0][0]             \n__________________________________________________________________________________________________\nconv2d_182 (Conv2D)             (None, 3, 3, 384)    442368      activation_180[0][0]             \n__________________________________________________________________________________________________\nconv2d_185 (Conv2D)             (None, 3, 3, 384)    442368      activation_184[0][0]             \n__________________________________________________________________________________________________\nconv2d_186 (Conv2D)             (None, 3, 3, 384)    442368      activation_184[0][0]             \n__________________________________________________________________________________________________\naverage_pooling2d_17 (AveragePo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n__________________________________________________________________________________________________\nconv2d_179 (Conv2D)             (None, 3, 3, 320)    655360      mixed9[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_181 (BatchN (None, 3, 3, 384)    1152        conv2d_181[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_182 (BatchN (None, 3, 3, 384)    1152        conv2d_182[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_185 (BatchN (None, 3, 3, 384)    1152        conv2d_185[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_186 (BatchN (None, 3, 3, 384)    1152        conv2d_186[0][0]                 \n__________________________________________________________________________________________________\nconv2d_187 (Conv2D)             (None, 3, 3, 192)    393216      average_pooling2d_17[0][0]       \n__________________________________________________________________________________________________\nbatch_normalization_179 (BatchN (None, 3, 3, 320)    960         conv2d_179[0][0]                 \n__________________________________________________________________________________________________\nactivation_181 (Activation)     (None, 3, 3, 384)    0           batch_normalization_181[0][0]    \n__________________________________________________________________________________________________\nactivation_182 (Activation)     (None, 3, 3, 384)    0           batch_normalization_182[0][0]    \n__________________________________________________________________________________________________\nactivation_185 (Activation)     (None, 3, 3, 384)    0           batch_normalization_185[0][0]    \n__________________________________________________________________________________________________\nactivation_186 (Activation)     (None, 3, 3, 384)    0           batch_normalization_186[0][0]    \n__________________________________________________________________________________________________\nbatch_normalization_187 (BatchN (None, 3, 3, 192)    576         conv2d_187[0][0]                 \n__________________________________________________________________________________________________\nactivation_179 (Activation)     (None, 3, 3, 320)    0           batch_normalization_179[0][0]    \n__________________________________________________________________________________________________\nmixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_181[0][0]             \n                                                                 activation_182[0][0]             \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 3, 3, 768)    0           activation_185[0][0]             \n                                                                 activation_186[0][0]             \n__________________________________________________________________________________________________\nactivation_187 (Activation)     (None, 3, 3, 192)    0           batch_normalization_187[0][0]    \n__________________________________________________________________________________________________\nmixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_179[0][0]             \n                                                                 mixed9_1[0][0]                   \n                                                                 concatenate_3[0][0]              \n                                                                 activation_187[0][0]             \n==================================================================================================\nTotal params: 21,802,784\nTrainable params: 0\nNon-trainable params: 21,802,784\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We can use smaller than the default 299x299x3 input for InceptionV3\n",
    "# which will speed up training. Keras v2.0.9 supports down to 139x139x3\n",
    "# In order to do so, we also must set include_top to False, which means the final fully-connected layer with 1,000 nodes for each ImageNet class is dropped, as well as a Global Average Pooling layer.\n",
    "\n",
    "input_size = 139\n",
    "model = InceptionV3(weights=weights_flag, include_top=False,\n",
    "                        input_shape=(input_size, input_size, 3))\n",
    "\n",
    "if freeze_flag == True:\n",
    "    for l in model.layers:\n",
    "        l.trainable = False\n",
    "    \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping layers\n",
    "You can drop layers from a model with `model.layers.pop()`."
   ]
  },
  {
   "source": [
    "### Attaching new input layer with Lambda layer to the pretrained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda\n",
    "import tensorflow as tf\n",
    "\n",
    "# Makes the input placeholder layer 32x32x3 for CIFAR-10\n",
    "cifar_input = Input(shape=(32,32,3))\n",
    "\n",
    "# Re-sizes the input with Kera's Lambda layer & attach to cifar_input\n",
    "resized_input = Lambda(lambda image: tf.image.resize( \n",
    "    image, (input_size, input_size)))(cifar_input)\n",
    "\n",
    "# Feeds the re-sized input into Inception model\n",
    "# You will need to update the model name if you changed it earlier!\n",
    "inp = model(resized_input)"
   ]
  },
  {
   "source": [
    "### Define Magic Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import random_ops\n",
    "\n",
    "\n",
    "class SMagicLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 filters=1,\n",
    "                 has_bias=True,\n",
    "                 activation=None,\n",
    "                 seed=None,\n",
    "                 **kwargs):\n",
    "        super(SMagicLayer, self).__init__(**kwargs)\n",
    "        self.has_bias = has_bias\n",
    "        self.seed = seed\n",
    "        self.activation = activation\n",
    "        self.filters = filters\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print('input_shape', input_shape)\n",
    "        w = input_shape[1]  # assuming input_shape=(None, w, h, c)\n",
    "        h = input_shape[2]\n",
    "        c = input_shape[3]\n",
    "        # self.kernels = []\n",
    "        # for _ in range(self.filters):\n",
    "        #     self.kernels.append(tf.Variable(trainable=True, name=self.name + '_kernel',\n",
    "        #                                     initial_value=random_ops.truncated_normal((w, h, c),\n",
    "        #                                                                               mean=0.0,\n",
    "        #                                                                               stddev=1.,\n",
    "        #                                                                               dtype=tf.float32,\n",
    "        #                                                                               seed=self.seed)))\n",
    "        self.kernel = tf.Variable(trainable=True, name=self.name + '_kernel',\n",
    "                                  initial_value=random_ops.truncated_normal((self.filters, w, h, c),\n",
    "                                                                            mean=0.0,\n",
    "                                                                            stddev=1.,\n",
    "                                                                            dtype=tf.float32,\n",
    "                                                                            seed=self.seed))\n",
    "\n",
    "        if self.has_bias:\n",
    "            self.bias = tf.Variable(trainable=True, name=self.name + '_bias',\n",
    "                                    initial_value=random_ops.truncated_normal((self.filters, w, h),\n",
    "                                                                              mean=0.0,\n",
    "                                                                              stddev=1.,\n",
    "                                                                              dtype=tf.float32,\n",
    "                                                                              seed=self.seed))\n",
    "        self.out_shape = (w, h, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print('inputs.shape', inputs.shape)\n",
    "        # print('self.kernel.shape', self.kernel.shape)\n",
    "        outputs = []\n",
    "        for f in range(self.filters):\n",
    "            o = tf.multiply(self.kernel[f, :, :], inputs)\n",
    "            o = tf.math.reduce_sum(o, axis=3)\n",
    "            if self.has_bias:\n",
    "                o = o + self.bias[f, :]\n",
    "            if self.activation is not None:\n",
    "                o = self.activation(o)\n",
    "            outputs.append(o)\n",
    "        # print('outputs.shape', outputs.shape)\n",
    "        # print('self.bias.shape', self.bias.shape)\n",
    "\n",
    "        return tf.stack(outputs, axis=3)\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Define ExpSq Activation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpSqActivation(tf.keras.layers.Layer):\n",
    "    def __init__(self, center=1.0, tau=0.45, bias=0.0):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.bias = bias\n",
    "        self.center = center\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = tf.exp(-1.0 * tf.square(self.center -\n",
    "                                    inputs) / (2.0 * self.tau ** 2))\n",
    "        if self.bias != 0:\n",
    "            y = (1-self.bias) * y + self.bias\n",
    "        return y\n"
   ]
  },
  {
   "source": [
    "### Connect your top layers to the predefined model + customized input layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_shape (None, 3, 3, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Imports fully-connected \"Dense\" layers & Global Average Pooling\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Conv2D, Softmax\n",
    "\n",
    "# out = GlobalAveragePooling2D()(inp)\n",
    "# out = Dense(400, activation='relu')(out)\n",
    "# predictions = Dense(10, activation='softmax', name='predictions')(out)\n",
    "\n",
    "# out = SMagicLayer(filters=10, activation=tf.keras.activations.relu)(inp)\n",
    "\n",
    "out = SMagicLayer(filters=1)(inp)\n",
    "# out = SMagicLayer(filters=10)(out)\n",
    "# out = SMagicLayer(filters=1)(out)\n",
    "# out = SMagicLayer(filters=1)(out)\n",
    "\n",
    "# my architecture\n",
    "# add conv full size here with 10 filters to generate 10 numbers\n",
    "# these numbers are kind of like pattern matching for each class\n",
    "# flatten()\n",
    "# softmax()\n",
    "# endof my architecture\n",
    "\n",
    "out = Flatten()(out)\n",
    "# out = Conv2D(filters=10, kernel_size=(3, 3))(out)\n",
    "#out = Flatten()(inp)\n",
    "out = Dense(10)(out)\n",
    "# out = Softmax()(out)\n",
    "out = Dense(10, activation='softmax')(out)\n"
   ]
  },
  {
   "source": [
    "### Compile the new model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 32, 32, 3)]       0         \n_________________________________________________________________\nlambda (Lambda)              (None, 139, 139, 3)       0         \n_________________________________________________________________\ninception_v3 (Functional)    (None, 3, 3, 2048)        21802784  \n_________________________________________________________________\ns_magic_layer_3 (SMagicLayer (None, 3, 3, 1)           18441     \n_________________________________________________________________\nflatten_5 (Flatten)          (None, 9)                 0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 10)                100       \n_________________________________________________________________\ndense_10 (Dense)             (None, 10)                110       \n=================================================================\nTotal params: 21,821,435\nTrainable params: 18,651\nNon-trainable params: 21,802,784\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Imports the Model API\n",
    "from keras.models import Model\n",
    "\n",
    "# Creates the model, assuming your final layer is named \"predictions\"\n",
    "new_model = Model(inputs=cifar_input, outputs=out)\n",
    "\n",
    "# Compile the model\n",
    "new_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Check the summary of this new model to confirm the architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks\n",
    "\n",
    "There's two key callbacks to mention here, `ModelCheckpoint` and `EarlyStopping`. As the names may suggest, model checkpoint saves down the best model so far based on a given metric, while early stopping will end training before the specified number of epochs if the chosen metric no longer improves after a given amount of time.\n",
    "\n",
    "To set these callbacks, you could do the following:\n",
    "```\n",
    "checkpoint = ModelCheckpoint(filepath=save_path, monitor='val_loss', save_best_only=True)\n",
    "```\n",
    "This would save a model to a specified `save_path`, based on validation loss, and only save down the best models. If you set `save_best_only` to `False`, every single epoch will save down another version of the model.\n",
    "```\n",
    "stopper = EarlyStopping(monitor='val_acc', min_delta=0.0003, patience=5)\n",
    "```\n",
    "This will monitor validation accuracy, and if it has not decreased by more than 0.0003 from the previous best validation accuracy for 5 epochs, training will end early.\n",
    "\n",
    "\n",
    "You still need to actually feed these callbacks into `fit()` when you train the model (along with all other relevant data to feed into `fit`):\n",
    "```\n",
    "model.fit(callbacks=[checkpoint, stopper])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "save_path = './checkpoints'\n",
    "checkpoint = ModelCheckpoint(filepath=save_path, monitor='val_loss', save_best_only=True)\n",
    "stopper = EarlyStopping(monitor='val_acc', min_delta=0.0003, patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
    "\n",
    "# One-hot encode the labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_one_hot_train = label_binarizer.fit_transform(y_train)\n",
    "y_one_hot_val = label_binarizer.fit_transform(y_val)\n",
    "\n",
    "# Shuffle the training & test data\n",
    "X_train, y_one_hot_train = shuffle(X_train, y_one_hot_train)\n",
    "X_val, y_one_hot_val = shuffle(X_val, y_one_hot_val)\n",
    "\n",
    "# We are only going to use the first 10,000 images for speed reasons\n",
    "# And only the first 2,000 images from the test set\n",
    "X_train = X_train[:10000]\n",
    "y_one_hot_train = y_one_hot_train[:10000]\n",
    "X_val = X_val[:2000]\n",
    "y_one_hot_val = y_one_hot_val[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out Keras's [ImageDataGenerator documentation](https://faroit.github.io/keras-docs/2.0.9/preprocessing/image/) for more information on the below - you can also add additional image augmentation through this function, although we are skipping that step here so you can potentially explore it in the upcoming project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a generator to pre-process our images for ImageNet\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "if preprocess_flag == True:\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "else:\n",
    "    datagen = ImageDataGenerator()\n",
    "    val_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "312/312 [==============================] - 12s 30ms/step - loss: 32.5096 - accuracy: 0.2960 - val_loss: 8.5364 - val_accuracy: 0.5720\n",
      "Epoch 2/5\n",
      "312/312 [==============================] - 9s 27ms/step - loss: 6.1511 - accuracy: 0.6069 - val_loss: 4.0317 - val_accuracy: 0.6330\n",
      "Epoch 3/5\n",
      "312/312 [==============================] - 8s 27ms/step - loss: 2.8178 - accuracy: 0.6515 - val_loss: 2.4009 - val_accuracy: 0.6395\n",
      "Epoch 4/5\n",
      "312/312 [==============================] - 8s 27ms/step - loss: 1.6413 - accuracy: 0.6729 - val_loss: 1.6042 - val_accuracy: 0.6495\n",
      "Epoch 5/5\n",
      "312/312 [==============================] - 9s 27ms/step - loss: 1.2574 - accuracy: 0.6776 - val_loss: 1.5196 - val_accuracy: 0.6300\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3635be47c0>"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "# Note: we aren't using callbacks here since we only are using 5 epochs to conserve GPU time\n",
    "new_model.fit(datagen.flow(X_train, y_one_hot_train, batch_size=batch_size), \n",
    "                    steps_per_epoch=len(X_train)/batch_size, epochs=epochs, verbose=1, \n",
    "                    validation_data=val_datagen.flow(X_val, y_one_hot_val, batch_size=batch_size),\n",
    "                    validation_steps=len(X_val)/batch_size)\n",
    "#                    callbacks=[checkpoint, stopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, CIFAR-10 is a fairly tough dataset. However, given that we are only training on a small subset of the data, only training for five epochs, and not using any image augmentation, the results are still fairly impressive!\n",
    "\n",
    "We achieved ~70% validation accuracy here, although your results may vary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90",
   "display_name": "Python 3.8.5 64-bit ('usr')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}